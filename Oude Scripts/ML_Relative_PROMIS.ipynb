{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new voorspelers file with semicolon separator\n",
    "voorspellers_df = pd.read_csv('../Export/Voorspellers.csv', delimiter=';')\n",
    "\n",
    "# Load the uitkomstmaat data_clean_cleanset\n",
    "uitkomstmaat_df = pd.read_csv('../Export/uitkomstmaat.csv', delimiter=';')\n",
    "uitkomstmaat_vars = uitkomstmaat_df[[\"patientnummer\", \"T0\", \"T5\"]]\n",
    "\n",
    "ML_df = pd.merge(voorspellers_df, uitkomstmaat_vars, left_on='Participant Id', right_on='patientnummer', how='inner')\n",
    "ML_df.drop(columns=['Participant Id', 'patientnummer'], inplace=True)\n",
    "ML_df['T5_relative'] = ML_df['T5']/ML_df['T0']\n",
    "ML_df['T5_recovered'] = ML_df['T5_relative'] >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of people below 0.9 and above 0.9\n",
    "below_count = ML_df[ML_df['T5_relative'] < 1].shape[0]\n",
    "above_count = ML_df[ML_df['T5_relative'] >= 1].shape[0]\n",
    "\n",
    "print(f\"Number of people with T5_relative below 0.9: {below_count}\")\n",
    "print(f\"Number of people with T5_relative 0.9 or above: {above_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assume ML_df is your DataFrame\n",
    "# data_clean is data met Imputation\n",
    "# data is data zonder imputation\n",
    "data_clean = ML_df.copy()\n",
    "data = ML_df.copy()\n",
    "\n",
    "# Identify columns\n",
    "continuous_columns = [\n",
    "    'T0_age', 'T0_BMI', 'T0_BIA_VVM_kg', 'T0_BIA_vetmassa_kg', \n",
    "    'Time_pretreat_OK', 'OK_Duration_min', 'Length_of_stay', \n",
    "    'T0_30SCST', 'T0_fatigue', 'T0_protein_perc', 'T0_kcal_perc', \n",
    "    'T0_CT_SMI', 'T0_CT_SMRA', 't0_gses_totaal_score', \n",
    "    'T0_participation_ability', 'T0_participation_satisfaction', \n",
    "    't0_EQ_5D_5L_beschrijvend-systeem_score', 'T0_pain', 'T0'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'T0_VVMI_per', 'Education', 'household', 'T0_Tumorsize', \n",
    "    'T0_diseaseburden_cat', 'T0_selfcare', 'T0_Locusofcontrol_cat',\n",
    "    'T0_socialsupport_cat', 'T0_coping_cat', 'AMEXO_8_day1', \n",
    "    'AMEXO_9_day2', 'AMEXO_10_day3', 'T0_sondevoeding', 'T0_protein_cat',\n",
    "    'T0_kcal_cat', 'T0_ASM_low', 'T0_anxiety_cat', 'T0_depression_cat'\n",
    "]\n",
    "\n",
    "# Remove rows with missing values in T5\n",
    "if 'T5_relative' in data_clean.columns:\n",
    "    data_clean = data_clean.dropna(subset=['T5_relative'])\n",
    "\n",
    "# Remove rows where T5 > 80\n",
    "if 'T5' in data_clean.columns:\n",
    "    data_clean = data_clean[data_clean['T5'] <= 80]\n",
    "\n",
    "# Initialize the SimpleImputer with median strategy for continuous variables\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply median imputation excluding 'T5'\n",
    "existing_continuous_columns = [col for col in continuous_columns if col in data_clean.columns]\n",
    "if existing_continuous_columns:\n",
    "    data_clean[existing_continuous_columns] = median_imputer.fit_transform(data_clean[existing_continuous_columns])\n",
    "\n",
    "# Initialize the SimpleImputer with mode strategy for categorical variables\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply mode imputation to categorical columns\n",
    "existing_categorical_columns = [col for col in categorical_columns if col in data_clean.columns]\n",
    "if existing_categorical_columns:\n",
    "    data_clean[existing_categorical_columns] = mode_imputer.fit_transform(data_clean[existing_categorical_columns])\n",
    "\n",
    "# Fill missing values for specific columns with 0\n",
    "for col in ['Complications_CCI', 'readmission_30days', 'Time_OK_posttreat']:\n",
    "    if col in data_clean.columns:\n",
    "        data_clean[col] = data_clean[col].fillna(0)\n",
    "\n",
    "# Verify no remaining missing values except for T5\n",
    "missing_values_final_check = data_clean.isnull().sum()\n",
    "data_clean\n",
    "\n",
    "# Output the missing values check\n",
    "missing_values_final_check.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Controleer op NaN of oneindige waarden en verwijder deze uit de kolom T5\n",
    "t5_cleaned = data_clean['T5_relative'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "t5_cleaned.hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Train / Test Split\n",
    "- Hier pak ik 4 modellen, LR, Lasso, XGboost en Nul model en ik train deze modellen op de test en kijk naar de nauwkeurigheid op de test set. Dit is nog zonder parameter optimalisatie. Alle modellen. Inclusief de lasso regressie lijken het slechter te doen dan de null model.\n",
    "- Daarna heb ik een RFE benadering geprobeerd met een lineare regressie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data_clean is already defined and clean\n",
    "\n",
    "# Select the features and target variable\n",
    "X = data_clean.drop(columns=['T5', 'T5_relative', 'T5_recovered'])\n",
    "y = data_clean['T5_relative']\n",
    "\n",
    "# Remove rows with NaN or infinite values in X or y\n",
    "mask = X.notna().all(axis=1) & np.isfinite(X).all(axis=1) & y.notna() & np.isfinite(y)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Perform a simple train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f'Number of patients in the test set: {len(y_test)}')\n",
    "\n",
    "# Initialize models\n",
    "linear_reg = LinearRegression()\n",
    "lasso_reg = Lasso(alpha=10, max_iter=10000)\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Train models\n",
    "linear_reg.fit(X_train, y_train)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on train and test sets\n",
    "y_train_pred_linear = linear_reg.predict(X_train)\n",
    "y_test_pred_linear = linear_reg.predict(X_test)\n",
    "\n",
    "y_train_pred_lasso = lasso_reg.predict(X_train)\n",
    "y_test_pred_lasso = lasso_reg.predict(X_test)\n",
    "\n",
    "y_train_pred_xgb = xgb_reg.predict(X_train)\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test)\n",
    "\n",
    "# Null model (predict the mean of the train set)\n",
    "y_train_pred_null = np.full_like(y_train, y_train.mean(), dtype=np.float64)\n",
    "y_test_pred_null = np.full_like(y_test, y_train.mean(), dtype=np.float64)\n",
    "\n",
    "# Define a function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Calculate metrics for each model on train and test sets\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Dataset': [],\n",
    "    'MAE': [],\n",
    "    'RMSE': [],\n",
    "    'R2': []\n",
    "}\n",
    "\n",
    "for model_name, y_train_pred, y_test_pred in [\n",
    "    ('Linear Regression', y_train_pred_linear, y_test_pred_linear),\n",
    "    ('Lasso Regression', y_train_pred_lasso, y_test_pred_lasso),\n",
    "    ('XGBoost Regression', y_train_pred_xgb, y_test_pred_xgb),\n",
    "    ('Null Model', y_train_pred_null, y_test_pred_null)\n",
    "]:\n",
    "    for dataset, y_true, y_pred in [\n",
    "        ('Train', y_train, y_train_pred),\n",
    "        ('Test', y_test, y_test_pred)\n",
    "    ]:\n",
    "        mae, rmse, r2 = calculate_metrics(y_true, y_pred)\n",
    "        metrics['Model'].append(model_name)\n",
    "        metrics['Dataset'].append(dataset)\n",
    "        metrics['MAE'].append(mae)\n",
    "        metrics['RMSE'].append(rmse)\n",
    "        metrics['R2'].append(r2)\n",
    "\n",
    "# Convert metrics to a DataFrame for easy viewing\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Select the features and target variable\n",
    "X = data_clean.drop(columns=['T5', 'T5_relative', 'T5_recovered'])\n",
    "y = data_clean['T5_relative']\n",
    "\n",
    "# Remove rows with NaN or infinite values in X or y\n",
    "mask = X.notna().all(axis=1) & np.isfinite(X).all(axis=1) & y.notna() & np.isfinite(y)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Perform a simple train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Recursive Feature Elimination based on p-values\n",
    "def recursive_feature_elimination(X, y, threshold=0.05):\n",
    "    X = X.copy()\n",
    "    while True:\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        pvalues = model.pvalues[1:]  # exclude the intercept\n",
    "        max_pvalue = pvalues.max()\n",
    "        if max_pvalue > threshold:\n",
    "            feature_to_remove = pvalues.idxmax()\n",
    "            X = X.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return X.columns[1:]  # exclude the constant\n",
    "\n",
    "# Perform RFE on the training set\n",
    "selected_features = recursive_feature_elimination(X_train, y_train)\n",
    "\n",
    "# Train a linear regression model with selected features\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Predict on train and test sets\n",
    "y_train_pred = linear_reg.predict(X_train[selected_features])\n",
    "y_test_pred = linear_reg.predict(X_test[selected_features])\n",
    "\n",
    "# Define a function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Calculate metrics for train and test sets\n",
    "train_mae, train_rmse, train_r2 = calculate_metrics(y_train, y_train_pred)\n",
    "test_mae, test_rmse, test_r2 = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(f'Train MAE: {train_mae}')\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Train R2: {train_r2}')\n",
    "print(f'Test MAE: {test_mae}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "print(f'Test R2: {test_r2}')\n",
    "\n",
    "# Add metrics to metrics_df\n",
    "new_metrics = {\n",
    "    'Model': ['Linear Regression - RFE', 'Linear Regression - RFE'],\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'MAE': [train_mae, test_mae],\n",
    "    'RMSE': [train_rmse, test_rmse],\n",
    "    'R2': [train_r2, test_r2]\n",
    "}\n",
    "\n",
    "new_metrics_df = pd.DataFrame(new_metrics)\n",
    "\n",
    "# Assuming metrics_df is already defined from previous models\n",
    "metrics_df = pd.concat([metrics_df, new_metrics_df], ignore_index=True)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation - train/validation set\n",
    "- Dataset wordt gesplitst in trainings- en testsets.\n",
    "- Er wordt een 5-voudige cross-validatie uitgevoerd met hyperparameter tuning voor zowel Lasso Regression als XGBoost Regression om de beste modelconfiguratie te vinden.\n",
    "- Het beste model wordt opnieuw getraind op de volledige trainingsset en geëvalueerd op de testset, waarbij de prestaties worden gemeten met MAE, RMSE en R².\n",
    "- Een null model, dat het gemiddelde van de trainingsset voorspelt, wordt gebruikt als benchmark om de prestaties van de regressiemodellen te vergelijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data_clean is already cleaned and imputed\n",
    "\n",
    "# Remove rows with missing values in T5\n",
    "data_clean = data_clean.dropna(subset=['T5_relative'])\n",
    "\n",
    "# Remove rows where T5 > 80\n",
    "data_clean = data_clean[data_clean['T5'] <= 80]\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data_clean.drop(columns=['T5', 'T5_relative', 'T5_recovered'])\n",
    "y = data_clean['T5_relative']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Perform 5-fold cross-validation with hyperparameter tuning\n",
    "def perform_cv_and_tuning(model, param_grid, X_train, y_train):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(max_iter=10000)\n",
    "lasso_param_grid = {'alpha': np.logspace(-4, 10, 50)}\n",
    "best_lasso = perform_cv_and_tuning(lasso, lasso_param_grid, X_train, y_train)\n",
    "\n",
    "# XGBoost Regression\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "best_xgb = perform_cv_and_tuning(xgb_reg, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "# Null Model\n",
    "class NullModel:\n",
    "    def fit(self, X, y):\n",
    "        self.mean = y.mean()\n",
    "    def predict(self, X):\n",
    "        return np.full(len(X), self.mean)\n",
    "\n",
    "null_model = NullModel()\n",
    "null_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Dataset': [],\n",
    "    'MAE': [],\n",
    "    'RMSE': [],\n",
    "    'R2': []\n",
    "}\n",
    "\n",
    "# Lasso Regression\n",
    "y_train_pred_lasso = best_lasso.predict(X_train)\n",
    "y_test_pred_lasso = best_lasso.predict(X_test)\n",
    "train_mae, train_rmse, train_r2 = calculate_metrics(y_train, y_train_pred_lasso)\n",
    "test_mae, test_rmse, test_r2 = calculate_metrics(y_test, y_test_pred_lasso)\n",
    "metrics['Model'].extend(['Lasso Regression', 'Lasso Regression'])\n",
    "metrics['Dataset'].extend(['Train', 'Test'])\n",
    "metrics['MAE'].extend([train_mae, test_mae])\n",
    "metrics['RMSE'].extend([train_rmse, test_rmse])\n",
    "metrics['R2'].extend([train_r2, test_r2])\n",
    "\n",
    "# XGBoost Regression\n",
    "y_train_pred_xgb = best_xgb.predict(X_train)\n",
    "y_test_pred_xgb = best_xgb.predict(X_test)\n",
    "train_mae, train_rmse, train_r2 = calculate_metrics(y_train, y_train_pred_xgb)\n",
    "test_mae, test_rmse, test_r2 = calculate_metrics(y_test, y_test_pred_xgb)\n",
    "metrics['Model'].extend(['XGBoost Regression', 'XGBoost Regression'])\n",
    "metrics['Dataset'].extend(['Train', 'Test'])\n",
    "metrics['MAE'].extend([train_mae, test_mae])\n",
    "metrics['RMSE'].extend([train_rmse, test_rmse])\n",
    "metrics['R2'].extend([train_r2, test_r2])\n",
    "\n",
    "# Null Model\n",
    "y_train_pred_null = null_model.predict(X_train)\n",
    "y_test_pred_null = null_model.predict(X_test)\n",
    "train_mae, train_rmse, train_r2 = calculate_metrics(y_train, y_train_pred_null)\n",
    "test_mae, test_rmse, test_r2 = calculate_metrics(y_test, y_test_pred_null)\n",
    "metrics['Model'].extend(['Null Model', 'Null Model'])\n",
    "metrics['Dataset'].extend(['Train', 'Test'])\n",
    "metrics['MAE'].extend([train_mae, test_mae])\n",
    "metrics['RMSE'].extend([train_rmse, test_rmse])\n",
    "metrics['R2'].extend([train_r2, test_r2])\n",
    "\n",
    "# Convert metrics to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Display the results\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intercept and coefficients from the best Lasso model\n",
    "lasso_intercept = best_lasso.intercept_\n",
    "lasso_coefficients = best_lasso.coef_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create the regression formula\n",
    "regression_formula = f\"y = {lasso_intercept:.4f}\"\n",
    "for coef, name in zip(lasso_coefficients, feature_names):\n",
    "    if coef != 0:  # Include only non-zero coefficients\n",
    "        regression_formula += f\" + ({coef:.4f} * {name})\"\n",
    "\n",
    "print(\"Lasso Regression Formula:\")\n",
    "print(regression_formula)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances from the best XGBoost model\n",
    "xgb_feature_importances = best_xgb.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(best_xgb)\n",
    "\n",
    "# Compute SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.title('Feature Importance with SHAP Values')\n",
    "plt.show()\n",
    "\n",
    "# SHAP summary plot with direction of association\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.title('SHAP Summary Plot')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
